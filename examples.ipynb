{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir/tfm/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torchvision\n",
    "from torchview import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\n",
    "    'maxvit_tiny_tf_224.in1k',\n",
    "    pretrained=True,\n",
    "    num_classes=0,  # remove classifier nn.Linear\n",
    "    no_jit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxxVit(\n",
       "  (stem): Stem(\n",
       "    (conv1): Conv2dSame(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (norm1): BatchNormAct2d(\n",
       "      64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): GELUTanh()\n",
       "    )\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): MaxxVitStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Downsample2d(\n",
       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "              (expand): Identity()\n",
       "            )\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): MaxxVitStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Downsample2d(\n",
       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "              (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): MaxxVitStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Downsample2d(\n",
       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "              (expand): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2dSame(1024, 1024, kernel_size=(3, 3), stride=(2, 2), groups=1024, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (2): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (3): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (4): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): MaxxVitStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Downsample2d(\n",
       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "              (expand): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2dSame(2048, 2048, kernel_size=(3, 3), stride=(2, 2), groups=2048, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): Identity()\n",
       "  (head): NormMlpClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n",
       "    (norm): LayerNorm2d((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (pre_logits): Sequential(\n",
       "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (act): Tanh()\n",
       "    )\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph = draw_graph(model, input_size=(1,3,224,224), expand_nested=True)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxvit_transformer import MaxVitTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaxVitTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxVitTransformer(\n",
       "  (model): MaxxVit(\n",
       "    (stem): Stem(\n",
       "      (conv1): Conv2dSame(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (norm1): BatchNormAct2d(\n",
       "        64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "        (drop): Identity()\n",
       "        (act): GELUTanh()\n",
       "      )\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (stages): Sequential(\n",
       "      (0): MaxxVitStage(\n",
       "        (blocks): Sequential(\n",
       "          (0): MaxxVitBlock(\n",
       "            (conv): MbConvBlock(\n",
       "              (shortcut): Downsample2d(\n",
       "                (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                (expand): Identity()\n",
       "              )\n",
       "              (pre_norm): BatchNormAct2d(\n",
       "                64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (down): Identity()\n",
       "              (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): BatchNormAct2d(\n",
       "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (conv2_kxk): Conv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
       "              (norm2): BatchNormAct2d(\n",
       "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (se): SEModule(\n",
       "                (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (bn): Identity()\n",
       "                (act): SiLU(inplace=True)\n",
       "                (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): MaxxVitBlock(\n",
       "            (conv): MbConvBlock(\n",
       "              (shortcut): Identity()\n",
       "              (pre_norm): BatchNormAct2d(\n",
       "                64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (down): Identity()\n",
       "              (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): BatchNormAct2d(\n",
       "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "              (norm2): BatchNormAct2d(\n",
       "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (se): SEModule(\n",
       "                (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (bn): Identity()\n",
       "                (act): SiLU(inplace=True)\n",
       "                (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): MaxxVitStage(\n",
       "        (blocks): Sequential(\n",
       "          (0): MaxxVitBlock(\n",
       "            (conv): MbConvBlock(\n",
       "              (shortcut): Downsample2d(\n",
       "                (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (pre_norm): BatchNormAct2d(\n",
       "                64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (down): Identity()\n",
       "              (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): BatchNormAct2d(\n",
       "                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (conv2_kxk): Conv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
       "              (norm2): BatchNormAct2d(\n",
       "                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (se): SEModule(\n",
       "                (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (bn): Identity()\n",
       "                (act): SiLU(inplace=True)\n",
       "                (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): MaxxVitBlock(\n",
       "            (conv): MbConvBlock(\n",
       "              (shortcut): Identity()\n",
       "              (pre_norm): BatchNormAct2d(\n",
       "                128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (down): Identity()\n",
       "              (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): BatchNormAct2d(\n",
       "                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (norm2): BatchNormAct2d(\n",
       "                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (se): SEModule(\n",
       "                (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (bn): Identity()\n",
       "                (act): SiLU(inplace=True)\n",
       "                (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): MaxxVitStage(\n",
       "        (blocks): Sequential(\n",
       "          (0): MaxxVitBlock(\n",
       "            (conv): MbConvBlock(\n",
       "              (shortcut): Downsample2d(\n",
       "                (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                (expand): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (pre_norm): BatchNormAct2d(\n",
       "                128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (down): Identity()\n",
       "              (conv1_1x1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): BatchNormAct2d(\n",
       "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (conv2_kxk): Conv2dSame(1024, 1024, kernel_size=(3, 3), stride=(2, 2), groups=1024, bias=False)\n",
       "              (norm2): BatchNormAct2d(\n",
       "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (se): SEModule(\n",
       "                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (bn): Identity()\n",
       "                (act): SiLU(inplace=True)\n",
       "                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): MaxxVitBlock(\n",
       "            (conv): MbConvBlock(\n",
       "              (shortcut): Identity()\n",
       "              (pre_norm): BatchNormAct2d(\n",
       "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (down): Identity()\n",
       "              (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): BatchNormAct2d(\n",
       "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (norm2): BatchNormAct2d(\n",
       "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (se): SEModule(\n",
       "                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (bn): Identity()\n",
       "                (act): SiLU(inplace=True)\n",
       "                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): MaxxVitBlock(\n",
       "            (conv): MbConvBlock(\n",
       "              (shortcut): Identity()\n",
       "              (pre_norm): BatchNormAct2d(\n",
       "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (down): Identity()\n",
       "              (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): BatchNormAct2d(\n",
       "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (norm2): BatchNormAct2d(\n",
       "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (se): SEModule(\n",
       "                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (bn): Identity()\n",
       "                (act): SiLU(inplace=True)\n",
       "                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): MaxxVitBlock(\n",
       "            (conv): MbConvBlock(\n",
       "              (shortcut): Identity()\n",
       "              (pre_norm): BatchNormAct2d(\n",
       "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (down): Identity()\n",
       "              (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): BatchNormAct2d(\n",
       "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (norm2): BatchNormAct2d(\n",
       "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (se): SEModule(\n",
       "                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (bn): Identity()\n",
       "                (act): SiLU(inplace=True)\n",
       "                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): MaxxVitBlock(\n",
       "            (conv): MbConvBlock(\n",
       "              (shortcut): Identity()\n",
       "              (pre_norm): BatchNormAct2d(\n",
       "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (down): Identity()\n",
       "              (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): BatchNormAct2d(\n",
       "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "              (norm2): BatchNormAct2d(\n",
       "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (se): SEModule(\n",
       "                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (bn): Identity()\n",
       "                (act): SiLU(inplace=True)\n",
       "                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): MaxxVitStage(\n",
       "        (blocks): Sequential(\n",
       "          (0): MaxxVitBlock(\n",
       "            (conv): MbConvBlock(\n",
       "              (shortcut): Downsample2d(\n",
       "                (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                (expand): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (pre_norm): BatchNormAct2d(\n",
       "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (down): Identity()\n",
       "              (conv1_1x1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): BatchNormAct2d(\n",
       "                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (conv2_kxk): Conv2dSame(2048, 2048, kernel_size=(3, 3), stride=(2, 2), groups=2048, bias=False)\n",
       "              (norm2): BatchNormAct2d(\n",
       "                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (se): SEModule(\n",
       "                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (bn): Identity()\n",
       "                (act): SiLU(inplace=True)\n",
       "                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): MaxxVitBlock(\n",
       "            (conv): MbConvBlock(\n",
       "              (shortcut): Identity()\n",
       "              (pre_norm): BatchNormAct2d(\n",
       "                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (down): Identity()\n",
       "              (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): BatchNormAct2d(\n",
       "                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "              (norm2): BatchNormAct2d(\n",
       "                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): GELUTanh()\n",
       "              )\n",
       "              (se): SEModule(\n",
       "                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (bn): Identity()\n",
       "                (act): SiLU(inplace=True)\n",
       "                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (rel_pos): RelPosBiasTf()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): Identity()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELUTanh()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): Identity()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Identity()\n",
       "    (head): NormMlpClassifierHead(\n",
       "      (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n",
       "      (norm): LayerNorm2d((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (pre_logits): Sequential(\n",
       "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (act): Tanh()\n",
       "      )\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (fc): Identity()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleList(\n",
       "    (0-13): 14 x Linear(in_features=512, out_features=384, bias=True)\n",
       "  )\n",
       "  (heads2): ModuleList(\n",
       "    (0-13): 14 x Linear(in_features=384, out_features=48, bias=True)\n",
       "  )\n",
       "  (heads3): ModuleList(\n",
       "    (0-13): 14 x Linear(in_features=48, out_features=48, bias=True)\n",
       "  )\n",
       "  (heads4): ModuleList(\n",
       "    (0-13): 14 x Linear(in_features=48, out_features=2, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand((2,3,224,224), dtype=torch.float32)\n",
    "output = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([2])) must be the same as input size (torch.Size([1, 2]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(output)):\n\u001b[0;32m---> 15\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_fn(output[i], targets[i]\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(loss)\n\u001b[1;32m     17\u001b[0m average_loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(output)\n",
      "File \u001b[0;32m~/tfm/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/tfm/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:720\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 720\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[1;32m    721\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    722\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[1;32m    723\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/tfm/venv/lib/python3.10/site-packages/torch/nn/functional.py:3163\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3160\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[0;32m-> 3163\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[1;32m   3165\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([2])) must be the same as input size (torch.Size([1, 2]))"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming you have the target values\n",
    "targets = torch.tensor([[1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1]])\n",
    "\n",
    "# Assuming you have the output\n",
    "# output = [torch.tensor([[-0.0249, 0.0991]]), torch.tensor([[-0.1355, -0.0978]]), torch.tensor([[0.1073, -0.0110]]), torch.tensor([[0.0873, -0.1508]]), torch.tensor([[-0.0300, 0.1163]]), torch.tensor([[0.0107, 0.0950]]), torch.tensor([[-0.0467, 0.0918]]), torch.tensor([[-0.0066, 0.0040]]), torch.tensor([[0.0597, -0.0886]]), torch.tensor([[-0.1488, -0.1139]]), torch.tensor([[0.1449, 0.1162]]), torch.tensor([[0.1001, 0.1511]]), torch.tensor([[-0.0551, 0.1024]]), torch.tensor([[-0.0745, -0.0954]])]\n",
    "\n",
    "# Calculate the binary cross-entropy loss\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss = 0\n",
    "\n",
    "for i in range(len(output)):\n",
    "    loss += loss_fn(output[i], targets[i].float())\n",
    "print(loss)\n",
    "average_loss = loss / len(output)\n",
    "\n",
    "print(\"Average Loss:\", average_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.0806, 0.1270],\n",
      "        [0.0844, 0.1221]], grad_fn=<AddmmBackward0>), tensor([[-0.1056,  0.0441],\n",
      "        [-0.1046,  0.0463]], grad_fn=<AddmmBackward0>), tensor([[0.0946, 0.0933],\n",
      "        [0.0883, 0.0912]], grad_fn=<AddmmBackward0>), tensor([[-0.0482,  0.0129],\n",
      "        [-0.0522,  0.0051]], grad_fn=<AddmmBackward0>), tensor([[-0.1348, -0.0177],\n",
      "        [-0.1380, -0.0189]], grad_fn=<AddmmBackward0>), tensor([[ 0.0539, -0.1293],\n",
      "        [ 0.0576, -0.1243]], grad_fn=<AddmmBackward0>), tensor([[ 0.0957, -0.0708],\n",
      "        [ 0.1138, -0.0735]], grad_fn=<AddmmBackward0>), tensor([[0.0951, 0.0719],\n",
      "        [0.0905, 0.0809]], grad_fn=<AddmmBackward0>), tensor([[ 0.0374, -0.0196],\n",
      "        [ 0.0503, -0.0029]], grad_fn=<AddmmBackward0>), tensor([[-0.0888,  0.0881],\n",
      "        [-0.1014,  0.0959]], grad_fn=<AddmmBackward0>), tensor([[-0.1705, -0.1477],\n",
      "        [-0.1481, -0.1370]], grad_fn=<AddmmBackward0>), tensor([[-0.0627,  0.0821],\n",
      "        [-0.0574,  0.0821]], grad_fn=<AddmmBackward0>), tensor([[0.1125, 0.0512],\n",
      "        [0.1004, 0.0488]], grad_fn=<AddmmBackward0>), tensor([[ 0.0268, -0.0620],\n",
      "        [ 0.0306, -0.0525]], grad_fn=<AddmmBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
