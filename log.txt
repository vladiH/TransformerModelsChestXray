=> merge config from configs/MAXVIT/maxvit_large_tf_384.in21k.yaml
RANK and WORLD_SIZE in environ: 0/1
[2023-07-05 13:06:48 maxvit_large_tf_384.in21k_ft_in1k](main.py 454): INFO Full config saved to output/maxvit_large_tf_384.in21k_ft_in1k/in21k/config.json
[2023-07-05 13:06:48 maxvit_large_tf_384.in21k_ft_in1k](main.py 457): INFO AMP_OPT_LEVEL: false
AUG:
  AUTO_AUGMENT: rand-m6-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0
  CUTMIX_MINMAX: null
  MIXUP: 0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 8
  CACHE_MODE: part
  DATASET: nih
  DATA_PATH: ''
  IMG_SIZE: 384
  INTERPOLATION: bicubic
  NUM_WORKERS: 16
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MAX_CHECKPOINTS: 3
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  MAXVIT:
    IN_CHANS: 3
  NAME: maxvit_large_tf_384.in21k_ft_in1k
  NUM_CLASSES: 14
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: maxvit
NIH:
  num_mlp_heads: 3
  test_csv_path: configs/NIH/test.csv
  testset: ../data/images/
  train_csv_path: configs/NIH/train.csv
  trainset: ../data/images/
  valid_csv_path: configs/NIH/validation.csv
  validset: ../data/images/
OUTPUT: output/maxvit_large_tf_384.in21k_ft_in1k/in21k
PRINT_FREQ: 100
SAVE_FREQ: 1
SEED: 0
TAG: in21k
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 4
  AUTO_RESUME: true
  BASE_LR: 0.000248125
  CLIP_GRAD: 5.0
  EARLYSTOPPING:
    MONITOR: auc
    PATIENCE: 8
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 25
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 3.125e-07
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 3.125e-08
  WEIGHT_DECAY: 0.08

local rank 0 / global rank 0 successfully build train dataset
local rank 0 / global rank 0 successfully build val and test dataset
[2023-07-05 13:06:48 maxvit_large_tf_384.in21k_ft_in1k](main.py 91): INFO Creating model:maxvit/maxvit_large_tf_384.in21k_ft_in1k
[2023-07-05 13:06:50 maxvit_large_tf_384.in21k_ft_in1k](main.py 94): INFO MaxVitTransformer(
  (model): MaxxVit(
    (stem): Stem(
      (conv1): Conv2dSame(3, 128, kernel_size=(3, 3), stride=(2, 2))
      (norm1): BatchNormAct2d(
        128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
        (drop): Identity()
        (act): GELUTanh()
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (stages): Sequential(
      (0): MaxxVitStage(
        (blocks): Sequential(
          (0): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Downsample2d(
                (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))
                (expand): Identity()
              )
              (pre_norm): BatchNormAct2d(
                128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)
              (norm2): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=128, out_features=128, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=128, out_features=512, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=512, out_features=128, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=128, out_features=128, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=128, out_features=512, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=512, out_features=128, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (1): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
              (norm2): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=128, out_features=128, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=128, out_features=512, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=512, out_features=128, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=128, out_features=128, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=128, out_features=512, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=512, out_features=128, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
        )
      )
      (1): MaxxVitStage(
        (blocks): Sequential(
          (0): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Downsample2d(
                (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))
                (expand): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
              )
              (pre_norm): BatchNormAct2d(
                128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2dSame(1024, 1024, kernel_size=(3, 3), stride=(2, 2), groups=1024, bias=False)
              (norm2): BatchNormAct2d(
                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (1): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
              (norm2): BatchNormAct2d(
                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (2): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
              (norm2): BatchNormAct2d(
                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (3): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
              (norm2): BatchNormAct2d(
                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (4): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
              (norm2): BatchNormAct2d(
                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (5): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
              (norm2): BatchNormAct2d(
                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
        )
      )
      (2): MaxxVitStage(
        (blocks): Sequential(
          (0): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Downsample2d(
                (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))
                (expand): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
              )
              (pre_norm): BatchNormAct2d(
                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2dSame(2048, 2048, kernel_size=(3, 3), stride=(2, 2), groups=2048, bias=False)
              (norm2): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (1): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)
              (norm2): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (2): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)
              (norm2): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (3): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)
              (norm2): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (4): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)
              (norm2): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (5): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)
              (norm2): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (6): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)
              (norm2): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (7): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)
              (norm2): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (8): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)
              (norm2): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (9): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)
              (norm2): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (10): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)
              (norm2): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (11): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)
              (norm2): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (12): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)
              (norm2): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (13): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)
              (norm2): BatchNormAct2d(
                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
        )
      )
      (3): MaxxVitStage(
        (blocks): Sequential(
          (0): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Downsample2d(
                (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))
                (expand): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
              )
              (pre_norm): BatchNormAct2d(
                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                4096, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2dSame(4096, 4096, kernel_size=(3, 3), stride=(2, 2), groups=4096, bias=False)
              (norm2): BatchNormAct2d(
                4096, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(256, 4096, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(4096, 1024, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
          (1): MaxxVitBlock(
            (conv): MbConvBlock(
              (shortcut): Identity()
              (pre_norm): BatchNormAct2d(
                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): Identity()
              )
              (down): Identity()
              (conv1_1x1): Conv2d(1024, 4096, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm1): BatchNormAct2d(
                4096, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (conv2_kxk): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4096, bias=False)
              (norm2): BatchNormAct2d(
                4096, eps=0.001, momentum=0.1, affine=True, track_running_stats=True
                (drop): Identity()
                (act): GELUTanh()
              )
              (se): SEModule(
                (fc1): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
                (bn): Identity()
                (act): SiLU(inplace=True)
                (fc2): Conv2d(256, 4096, kernel_size=(1, 1), stride=(1, 1))
                (gate): Sigmoid()
              )
              (conv3_1x1): Conv2d(4096, 1024, kernel_size=(1, 1), stride=(1, 1))
              (drop_path): Identity()
            )
            (attn_block): PartitionAttentionCl(
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
            (attn_grid): PartitionAttentionCl(
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (attn): AttentionCl(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (rel_pos): RelPosBiasTf()
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
              (ls1): Identity()
              (drop_path1): Identity()
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (act): GELUTanh()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (ls2): Identity()
              (drop_path2): Identity()
            )
          )
        )
      )
    )
    (norm): Identity()
    (head): NormMlpClassifierHead(
      (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())
      (norm): LayerNorm2d((1024,), eps=1e-05, elementwise_affine=True)
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (pre_logits): Sequential(
        (fc): Linear(in_features=1024, out_features=1024, bias=True)
        (act): Tanh()
      )
      (drop): Dropout(p=0.0, inplace=False)
      (fc): Identity()
    )
  )
  (heads): ModuleList(
    (0-13): 14 x Linear(in_features=1024, out_features=384, bias=True)
  )
  (heads2): ModuleList(
    (0-13): 14 x Linear(in_features=384, out_features=48, bias=True)
  )
  (heads3): ModuleList(
    (0-13): 14 x Linear(in_features=48, out_features=48, bias=True)
  )
  (heads4): ModuleList(
    (0-13): 14 x Linear(in_features=48, out_features=2, bias=True)
  )
  (relu): ReLU()
)
[2023-07-05 13:06:50 maxvit_large_tf_384.in21k_ft_in1k](main.py 101): INFO number of params: 216811660
All checkpoints founded in output/maxvit_large_tf_384.in21k_ft_in1k/in21k: []
[2023-07-05 13:06:50 maxvit_large_tf_384.in21k_ft_in1k](main.py 128): INFO no checkpoint found in output/maxvit_large_tf_384.in21k_ft_in1k/in21k, ignoring auto resume
[2023-07-05 13:06:50 maxvit_large_tf_384.in21k_ft_in1k](main.py 148): INFO Start training
[2023-07-05 13:06:50 maxvit_large_tf_384.in21k_ft_in1k](main.py 150): INFO Early Stopping is monitoring: auc
[2023-07-05 13:06:54 maxvit_large_tf_384.in21k_ft_in1k](main.py 272): INFO Train: [0/300][0/8679]	eta 9:00:40 lr 0.000000	time 3.7378 (3.7378)	loss 2.4787 (2.4787)	grad_norm 0.0000 (0.0000)	mem 17962MB
[2023-07-05 13:07:14 maxvit_large_tf_384.in21k_ft_in1k](main.py 272): INFO Train: [0/300][100/8679]	eta 0:33:43 lr 0.000000	time 0.1907 (0.2358)	loss 2.4726 (2.4789)	grad_norm 2.7819 (2.7696)	mem 18209MB
[2023-07-05 13:07:34 maxvit_large_tf_384.in21k_ft_in1k](main.py 272): INFO Train: [0/300][200/8679]	eta 0:30:46 lr 0.000000	time 0.1907 (0.2177)	loss 2.4806 (2.4788)	grad_norm 2.7774 (2.8245)	mem 18209MB
[2023-07-05 13:07:54 maxvit_large_tf_384.in21k_ft_in1k](main.py 272): INFO Train: [0/300][300/8679]	eta 0:29:33 lr 0.000000	time 0.1907 (0.2117)	loss 2.4743 (2.4776)	grad_norm 2.8690 (2.8332)	mem 18209MB
[2023-07-05 13:08:14 maxvit_large_tf_384.in21k_ft_in1k](main.py 272): INFO Train: [0/300][400/8679]	eta 0:28:47 lr 0.000001	time 0.1909 (0.2087)	loss 2.4773 (2.4760)	grad_norm 2.9991 (2.8437)	mem 18209MB
[2023-07-05 13:08:34 maxvit_large_tf_384.in21k_ft_in1k](main.py 272): INFO Train: [0/300][500/8679]	eta 0:28:12 lr 0.000001	time 0.1908 (0.2069)	loss 2.4712 (2.4739)	grad_norm 2.8931 (2.8446)	mem 18209MB
[2023-07-05 13:08:54 maxvit_large_tf_384.in21k_ft_in1k](main.py 272): INFO Train: [0/300][600/8679]	eta 0:27:42 lr 0.000001	time 0.1908 (0.2058)	loss 2.4479 (2.4718)	grad_norm 2.9216 (2.8491)	mem 18209MB
[2023-07-05 13:09:14 maxvit_large_tf_384.in21k_ft_in1k](main.py 272): INFO Train: [0/300][700/8679]	eta 0:27:14 lr 0.000001	time 0.1909 (0.2049)	loss 2.4453 (2.4693)	grad_norm 2.8109 (2.8522)	mem 18209MB
[2023-07-05 13:09:34 maxvit_large_tf_384.in21k_ft_in1k](main.py 272): INFO Train: [0/300][800/8679]	eta 0:26:49 lr 0.000001	time 0.1908 (0.2042)	loss 2.4233 (2.4661)	grad_norm 2.9435 (2.8493)	mem 18209MB
